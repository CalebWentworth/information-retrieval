{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IwelM4d4fWgX"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6V0d_awCfcYC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model_id = 'naver/splade_v2_max'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gUs0sMVRfeos"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskedLMOutput(loss=None, logits=tensor([[[ -8.6903,  -8.6712,  -8.7282,  ...,  -7.7577,  -7.6470,  -8.4183],\n",
      "         [-12.9129, -12.7483, -12.8099,  ..., -11.4523, -11.3861, -12.9494],\n",
      "         [-10.7632, -10.4263, -10.6957,  ...,  -9.3885,  -9.1583, -12.1823],\n",
      "         ...,\n",
      "         [-13.8963, -14.0878, -14.1625,  ..., -11.4821, -12.5815, -12.8123],\n",
      "         [-12.7274, -12.8395, -12.9904,  ..., -11.2488, -11.8553, -10.9621],\n",
      "         [-10.6878, -10.6928, -10.7399,  ...,  -8.6930,  -8.7811,  -9.7368]]],\n",
      "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "torch.Size([1, 11, 30522])\n",
      "torch.Size([30522])\n"
     ]
    }
   ],
   "source": [
    "text = \"Information Retrieval course at the University of Southern Maine\"\n",
    "tokens = tokenizer(text, return_tensors='pt')\n",
    "output = model(**tokens)\n",
    "print(output)\n",
    "print(output.logits.shape)\n",
    "import torch\n",
    "\n",
    "vec = torch.max(torch.log(1 + torch.relu(output.logits)\n",
    "    ) * tokens.attention_mask.unsqueeze(-1),\n",
    "dim=1)[0].squeeze()\n",
    "# Get SPLADE Vector\n",
    "print(vec.shape) #torch.Size([30522])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EhKpOeJIfsQc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "# extract non-zero positions\n",
    "cols = vec.nonzero().squeeze().cpu().tolist()\n",
    "print(len(cols))\n",
    "\n",
    "# extract the non-zero values\n",
    "weights = vec[cols].cpu().tolist()\n",
    "# use to create a dictionary of token ID to weight\n",
    "sparse_dict = dict(zip(cols, weights))\n",
    "sparse_dict\n",
    "\n",
    "# extract the ID position to text token mappings\n",
    "idx2token = {\n",
    "    idx: token for token, idx in tokenizer.get_vocab().items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "te_xwRZ3fuu7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'retrieval': 2.27,\n",
       " 'information': 2.13,\n",
       " 'southern': 2.11,\n",
       " 'maine': 2.03,\n",
       " 'university': 1.6,\n",
       " 'course': 1.54,\n",
       " 'retrieve': 1.35,\n",
       " 'south': 1.34,\n",
       " 'recall': 1.16,\n",
       " 'retrieved': 1.14,\n",
       " 'attend': 1.06,\n",
       " 'massachusetts': 1.04,\n",
       " 'bangor': 0.76,\n",
       " 'remember': 0.73,\n",
       " 'at': 0.7,\n",
       " 'reference': 0.7,\n",
       " 'uc': 0.7,\n",
       " 'northern': 0.66,\n",
       " 'library': 0.6,\n",
       " 'info': 0.59,\n",
       " 'school': 0.58,\n",
       " 'open': 0.52,\n",
       " 'degree': 0.48,\n",
       " 'recognition': 0.47,\n",
       " 'portland': 0.39,\n",
       " 'key': 0.35,\n",
       " 'stream': 0.34,\n",
       " 'southwest': 0.33,\n",
       " 'columbia': 0.3,\n",
       " 'search': 0.27,\n",
       " 'southeast': 0.26,\n",
       " 'where': 0.23,\n",
       " 'college': 0.2,\n",
       " 'attended': 0.17,\n",
       " 'u': 0.12,\n",
       " 'data': 0.1,\n",
       " 'processing': 0.08,\n",
       " 'education': 0.05,\n",
       " 'memory': 0.04,\n",
       " 'guide': 0.03,\n",
       " 'samuel': 0.02,\n",
       " 'access': 0.01,\n",
       " 'florida': 0.01}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map token IDs to human-readable tokens\n",
    "sparse_dict_tokens = {\n",
    "    idx2token[idx]: round(weight, 2) for idx, weight in zip(cols, weights)\n",
    "}\n",
    "# sort so we can see most relevant tokens first\n",
    "sparse_dict_tokens = {\n",
    "    k: v for k, v in sorted(\n",
    "        sparse_dict_tokens.items(),\n",
    "        key=lambda item: item[1],\n",
    "        reverse=True\n",
    "    )\n",
    "}\n",
    "sparse_dict_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vZSUj1AHfv9u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3208], grad_fn=<SumBackward1>)\n",
      "tensor([0.0733], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "text1 = \"Information Retrieval course at the University of Southern Maine\"\n",
    "text2 = \"Courses about search engines in Maine\"\n",
    "text3 = \"Courses to avoid in computer science\"\n",
    "tokens1 = tokenizer(text1, return_tensors='pt')\n",
    "tokens2 = tokenizer(text2, return_tensors='pt')\n",
    "tokens3 = tokenizer(text3, return_tensors='pt')\n",
    "\n",
    "output = model(**tokens1)\n",
    "vec1 = torch.max(torch.log(1 + torch.relu(output.logits)\n",
    "    ) * tokens1.attention_mask.unsqueeze(-1),dim=1)[0].squeeze().view(-1, 1)\n",
    "\n",
    "output = model(**tokens2)\n",
    "vec2 = torch.max(torch.log(1 + torch.relu(output.logits)\n",
    "    ) * tokens2.attention_mask.unsqueeze(-1),dim=1)[0].squeeze().view(-1, 1)\n",
    "\n",
    "output = model(**tokens3)\n",
    "vec3 = torch.max(torch.log(1 + torch.relu(output.logits)\n",
    "    ) * tokens3.attention_mask.unsqueeze(-1),dim=1)[0].squeeze().view(-1, 1)\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "output = cos(vec1, vec2)\n",
    "print(output)\n",
    "output = cos(vec1, vec3)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
