{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwelM4d4fWgX"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "\n",
        "model_id = 'naver/splade_v2_max'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "6V0d_awCfcYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Information Retrieval course at the University of Southern Maine\"\n",
        "tokens = tokenizer(text, return_tensors='pt')\n",
        "output = model(**tokens)\n",
        "print(output)\n",
        "print(output.logits.shape)\n",
        "import torch\n",
        "\n",
        "vec = torch.max(torch.log(1 + torch.relu(output.logits)\n",
        "    ) * tokens.attention_mask.unsqueeze(-1),\n",
        "dim=1)[0].squeeze()\n",
        "# Get SPLADE Vector\n",
        "print(vec.shape) #torch.Size([30522])\n"
      ],
      "metadata": {
        "id": "gUs0sMVRfeos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract non-zero positions\n",
        "cols = vec.nonzero().squeeze().cpu().tolist()\n",
        "print(len(cols))\n",
        "\n",
        "# extract the non-zero values\n",
        "weights = vec[cols].cpu().tolist()\n",
        "# use to create a dictionary of token ID to weight\n",
        "sparse_dict = dict(zip(cols, weights))\n",
        "sparse_dict\n",
        "\n",
        "# extract the ID position to text token mappings\n",
        "idx2token = {\n",
        "    idx: token for token, idx in tokenizer.get_vocab().items()\n",
        "}\n"
      ],
      "metadata": {
        "id": "EhKpOeJIfsQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map token IDs to human-readable tokens\n",
        "sparse_dict_tokens = {\n",
        "    idx2token[idx]: round(weight, 2) for idx, weight in zip(cols, weights)\n",
        "}\n",
        "# sort so we can see most relevant tokens first\n",
        "sparse_dict_tokens = {\n",
        "    k: v for k, v in sorted(\n",
        "        sparse_dict_tokens.items(),\n",
        "        key=lambda item: item[1],\n",
        "        reverse=True\n",
        "    )\n",
        "}\n",
        "sparse_dict_tokens\n"
      ],
      "metadata": {
        "id": "te_xwRZ3fuu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "text1 = \"Information Retrieval course at the University of Southern Maine\"\n",
        "text2 = \"Courses about search engines in Maine\"\n",
        "text3 = \"Courses to avoid in computer science\"\n",
        "tokens1 = tokenizer(text1, return_tensors='pt')\n",
        "tokens2 = tokenizer(text2, return_tensors='pt')\n",
        "tokens3 = tokenizer(text3, return_tensors='pt')\n",
        "\n",
        "output = model(**tokens1)\n",
        "vec1 = torch.max(torch.log(1 + torch.relu(output.logits)\n",
        "    ) * tokens1.attention_mask.unsqueeze(-1),dim=1)[0].squeeze().view(-1, 1)\n",
        "\n",
        "output = model(**tokens2)\n",
        "vec2 = torch.max(torch.log(1 + torch.relu(output.logits)\n",
        "    ) * tokens2.attention_mask.unsqueeze(-1),dim=1)[0].squeeze().view(-1, 1)\n",
        "\n",
        "output = model(**tokens3)\n",
        "vec3 = torch.max(torch.log(1 + torch.relu(output.logits)\n",
        "    ) * tokens3.attention_mask.unsqueeze(-1),dim=1)[0].squeeze().view(-1, 1)\n",
        "\n",
        "cos = torch.nn.CosineSimilarity(dim=0)\n",
        "output = cos(vec1, vec2)\n",
        "print(output)\n",
        "output = cos(vec1, vec3)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "vZSUj1AHfv9u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}