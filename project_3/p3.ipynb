{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from argparse import ArgumentParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(string):\n",
    "    cleantext = BeautifulSoup(string, \"lxml\").text\n",
    "    return cleantext\n",
    "\n",
    "def load_topic_file(topic_filepath):\n",
    "    # a method used to read the topic file for this year of the lab; to be passed to BERT/PyTerrier methods\n",
    "    queries = json.load(open(topic_filepath))\n",
    "    result = {}\n",
    "    for item in queries:\n",
    "      # returing results as dictionary of topic id: [title, body, tag]\n",
    "      title =  item['Title'].translate(str.maketrans('', '', string.punctuation))\n",
    "      #removing html from body\n",
    "      body = remove_html(item['Body']).translate(str.maketrans('', '', string.punctuation))\n",
    "      tags = item['Tags']\n",
    "      result[item['Id']] = [title, body, tags]\n",
    "    return result\n",
    "\n",
    "def read_qrel_file(qrel_filepath):\n",
    "    # a method used to read the topic file\n",
    "    result = {}\n",
    "    with open(qrel_filepath, \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter='\\t', lineterminator='\\n')\n",
    "        for line in reader:\n",
    "            query_id = line[0]\n",
    "            doc_id = line[2]\n",
    "            score = int(line[3])\n",
    "            if query_id in result:\n",
    "                result[query_id][doc_id] = score\n",
    "            else:\n",
    "                result[query_id] = {doc_id: score}\n",
    "    # dictionary of key:query_id value: dictionary of key:doc id value: score\n",
    "    return result\n",
    "\n",
    "def read_collection(answer_filepath):\n",
    "  # Reading collection to a dictionary\n",
    "  lst = json.load(open(answer_filepath))\n",
    "  result = {}\n",
    "  for doc in lst:\n",
    "    #processes the answers to remove html and punctuation.\n",
    "    result[doc['Id']] = remove_html(doc['Text']).translate(str.maketrans('', '', string.punctuation))\n",
    "  return result\n",
    "#modifies each query to contain the title and body\n",
    "def prep_queries(topics):\n",
    "    queries = {}\n",
    "    for query_id in topics:\n",
    "        queries[query_id] = \"[TITLE]\" + topics[query_id][0] + \"[BODY]\" + topics[query_id][1]\n",
    "    return queries\n",
    "\n",
    "from sentence_transformers import InputExample\n",
    "\n",
    "def cross_dataset_gen(queries,qrel,answers):\n",
    "    result_list = []\n",
    "    sample_list =[]\n",
    "    for topic in qrel:\n",
    "        print(f\"Key: {topic}, Value: {qrel[topic]}\")\n",
    "        for doc, score in qrel[topic].items():\n",
    "            pair = (queries[topic],answers[doc],score)\n",
    "            result_list.append(pair)\n",
    "\n",
    "    for pair in result_list:\n",
    "        ex_1 = pair[0]\n",
    "        ex_2 = pair[1]\n",
    "        label = pair[2]\n",
    "        if label >=1:\n",
    "            label = 1\n",
    "        sample_list.append(InputExample(texts=[pair[0],pair[1]],label=label))\n",
    "\n",
    "    return sample_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def main():\\n    parser = ArgumentParser()\\n    parser.add_argument(\\'-i\\', \\'--input\\', required=True, help=\\'search domain file e.g. Answers.json\\', default=\"Answers.json\")\\n    parser.add_argument(\\'-t\\', \\'--topic\\', required=True, help=\\'topic source files e.g. topics_1.json\\', default=\"topics_1.json\")\\n    parser.add_argument(\\'-q\\', \\'--qrel\\', required=True, help=\\'qrel source files e.g. qrel_1.tsv\\', default=\"qrel_1.tsv\")\\n    args = parser.parse_args()\\n    answer_filepath = args.input\\n    topic_filepath = args.topic\\n    qrel_filepath = args.qrel\\n\\n    topics = load_topic_file(topic_filepath)\\n    qrel = read_qrel_file(qrel_filepath)\\n    answers = read_collection(answer_filepath)\\n\\n    queries = prep_queries(topics)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def main():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('-i', '--input', required=True, help='search domain file e.g. Answers.json', default=\"Answers.json\")\n",
    "    parser.add_argument('-t', '--topic', required=True, help='topic source files e.g. topics_1.json', default=\"topics_1.json\")\n",
    "    parser.add_argument('-q', '--qrel', required=True, help='qrel source files e.g. qrel_1.tsv', default=\"qrel_1.tsv\")\n",
    "    args = parser.parse_args()\n",
    "    answer_filepath = args.input\n",
    "    topic_filepath = args.topic\n",
    "    qrel_filepath = args.qrel\n",
    "\n",
    "    topics = load_topic_file(topic_filepath)\n",
    "    qrel = read_qrel_file(qrel_filepath)\n",
    "    answers = read_collection(answer_filepath)\n",
    "\n",
    "    queries = prep_queries(topics)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = load_topic_file(\"topics_1.json\")\n",
    "qrel = read_qrel_file(\"qrel_1.tsv\")\n",
    "answers = read_collection(\"Answers.json\")\n",
    "queries = prep_queries(topics)\n",
    "#print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55234df5fa8e4a29a209341a4b03c364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "pipe = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "system_prompt = {\"role\": \"System\",\"content\": \"\"\"You are an expert at answering ubuntu related questions on stackoverflow the question \n",
    "                and answer website, given a question you craft an suitable answer in plain text with no markdown formatting.\"\"\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_new_queries(queries, system_prompt,pipe): \n",
    "    #quids = queries.keys()\n",
    "    #list_queries = list(queries.values())\n",
    "\n",
    "    #this will take in each query, pass its combined query to the LLM which will rewrite the query into an answer\n",
    "    #We do this so as to allow semantic searching answer -> answer rather than question -> answer.\n",
    "    #we then export to TSV\n",
    "\n",
    "    for query_id in queries:\n",
    "        query_input = {\"role\": \"user\", \"content\": queries[query_id]}\n",
    "        messages = [system_prompt,query_input]\n",
    "        prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        terminators = [pipe.tokenizer.eos_token_id, pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "        outputs = pipe(prompt, max_new_tokens=256, eos_token_id=terminators, do_sample=True, temperature=0.6,\n",
    "        top_p=0.9, pad_token_id = pipe.tokenizer.eos_token_id)\n",
    "        result= outputs[0][\"generated_text\"][len(prompt):].strip()\n",
    "        queries[query_id] = result\n",
    "        print(\"processed qid \"+query_id)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_answers = gen_new_queries(queries,system_prompt,pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_q = {k: [v] for k, v in query_answers.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "qd = pd.DataFrame.from_dict(tmp_q,orient='index', columns=['Content'])\n",
    "qd.reset_index(inplace=True)\n",
    "qd.rename(columns={'index': 'DocID'}, inplace=True)\n",
    "qd.to_csv('queries.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = pd.DataFrame.from_dict(answers,orient='index', columns=['Content'])\n",
    "ad.reset_index(inplace=True)\n",
    "ad.rename(columns={'index': 'DocID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using ChromaDB as a vecotr database for searching, it uses sbert as a text tokenizer.\n",
    "%pip install chromadb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/caleb/school/information-retrieval/project_3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "db_path = cwd+\"/chroma\"\n",
    "client = chromadb.PersistentClient(path=db_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we select the embedding function to use with the DB\n",
    "#we can also use local models\n",
    "from chromadb.utils import embedding_functions\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.create_collection(name=\"answers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(lst, batch_size):\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1\n",
      "batch: 2\n",
      "batch: 3\n",
      "batch: 4\n",
      "batch: 5\n",
      "batch: 6\n",
      "batch: 7\n",
      "batch: 8\n",
      "batch: 9\n",
      "batch: 10\n",
      "batch: 11\n",
      "batch: 12\n",
      "batch: 13\n",
      "batch: 14\n",
      "batch: 15\n",
      "batch: 16\n",
      "batch: 17\n",
      "batch: 18\n",
      "batch: 19\n",
      "batch: 20\n",
      "batch: 21\n",
      "batch: 22\n",
      "batch: 23\n",
      "batch: 24\n",
      "batch: 25\n",
      "batch: 26\n",
      "batch: 27\n",
      "batch: 28\n",
      "batch: 29\n",
      "batch: 30\n",
      "batch: 31\n",
      "batch: 32\n",
      "batch: 33\n",
      "batch: 34\n",
      "batch: 35\n",
      "batch: 36\n",
      "batch: 37\n",
      "batch: 38\n",
      "batch: 39\n",
      "batch: 40\n",
      "batch: 41\n",
      "batch: 42\n",
      "batch: 43\n",
      "batch: 44\n",
      "batch: 45\n",
      "batch: 46\n",
      "batch: 47\n",
      "batch: 48\n",
      "batch: 49\n",
      "batch: 50\n",
      "batch: 51\n",
      "batch: 52\n",
      "batch: 53\n",
      "batch: 54\n",
      "batch: 55\n",
      "batch: 56\n",
      "batch: 57\n",
      "batch: 58\n",
      "batch: 59\n",
      "batch: 60\n",
      "batch: 61\n",
      "batch: 62\n",
      "batch: 63\n",
      "batch: 64\n",
      "batch: 65\n",
      "batch: 66\n",
      "batch: 67\n",
      "batch: 68\n",
      "batch: 69\n",
      "batch: 70\n",
      "batch: 71\n",
      "batch: 72\n",
      "batch: 73\n",
      "batch: 74\n",
      "batch: 75\n",
      "batch: 76\n",
      "batch: 77\n",
      "batch: 78\n",
      "batch: 79\n",
      "batch: 80\n",
      "batch: 81\n",
      "batch: 82\n",
      "batch: 83\n",
      "batch: 84\n",
      "batch: 85\n",
      "batch: 86\n",
      "batch: 87\n",
      "batch: 88\n",
      "batch: 89\n",
      "batch: 90\n",
      "batch: 91\n",
      "batch: 92\n",
      "batch: 93\n",
      "batch: 94\n",
      "batch: 95\n",
      "batch: 96\n",
      "batch: 97\n",
      "batch: 98\n",
      "batch: 99\n",
      "batch: 100\n",
      "batch: 101\n",
      "batch: 102\n",
      "batch: 103\n",
      "batch: 104\n",
      "batch: 105\n",
      "batch: 106\n",
      "batch: 107\n",
      "batch: 108\n",
      "batch: 109\n",
      "batch: 110\n",
      "batch: 111\n",
      "batch: 112\n",
      "batch: 113\n",
      "batch: 114\n",
      "batch: 115\n",
      "batch: 116\n",
      "batch: 117\n",
      "batch: 118\n",
      "batch: 119\n",
      "batch: 120\n",
      "batch: 121\n",
      "batch: 122\n",
      "batch: 123\n",
      "batch: 124\n",
      "batch: 125\n",
      "batch: 126\n",
      "batch: 127\n",
      "batch: 128\n",
      "batch: 129\n",
      "batch: 130\n",
      "batch: 131\n",
      "batch: 132\n",
      "batch: 133\n",
      "batch: 134\n",
      "batch: 135\n",
      "batch: 136\n",
      "batch: 137\n",
      "batch: 138\n",
      "batch: 139\n",
      "batch: 140\n",
      "batch: 141\n",
      "batch: 142\n",
      "batch: 143\n",
      "batch: 144\n",
      "batch: 145\n",
      "batch: 146\n",
      "batch: 147\n",
      "batch: 148\n",
      "batch: 149\n",
      "batch: 150\n",
      "batch: 151\n",
      "batch: 152\n",
      "batch: 153\n",
      "batch: 154\n",
      "batch: 155\n",
      "batch: 156\n",
      "batch: 157\n",
      "batch: 158\n",
      "batch: 159\n",
      "batch: 160\n",
      "batch: 161\n",
      "batch: 162\n",
      "batch: 163\n",
      "batch: 164\n",
      "batch: 165\n",
      "batch: 166\n",
      "batch: 167\n",
      "batch: 168\n",
      "batch: 169\n",
      "batch: 170\n",
      "batch: 171\n",
      "batch: 172\n",
      "batch: 173\n",
      "batch: 174\n",
      "batch: 175\n",
      "batch: 176\n",
      "batch: 177\n",
      "batch: 178\n",
      "batch: 179\n",
      "batch: 180\n",
      "batch: 181\n",
      "batch: 182\n",
      "batch: 183\n",
      "batch: 184\n",
      "batch: 185\n",
      "batch: 186\n",
      "batch: 187\n",
      "batch: 188\n",
      "batch: 189\n",
      "batch: 190\n",
      "batch: 191\n",
      "batch: 192\n",
      "batch: 193\n",
      "batch: 194\n",
      "batch: 195\n",
      "batch: 196\n",
      "batch: 197\n",
      "batch: 198\n",
      "batch: 199\n",
      "batch: 200\n",
      "batch: 201\n",
      "batch: 202\n",
      "batch: 203\n",
      "batch: 204\n",
      "batch: 205\n",
      "batch: 206\n",
      "batch: 207\n",
      "batch: 208\n",
      "batch: 209\n",
      "batch: 210\n",
      "batch: 211\n",
      "batch: 212\n",
      "batch: 213\n",
      "batch: 214\n",
      "batch: 215\n",
      "batch: 216\n",
      "batch: 217\n",
      "batch: 218\n",
      "batch: 219\n",
      "batch: 220\n",
      "batch: 221\n",
      "batch: 222\n",
      "batch: 223\n",
      "batch: 224\n",
      "batch: 225\n",
      "batch: 226\n",
      "batch: 227\n",
      "batch: 228\n",
      "batch: 229\n",
      "batch: 230\n",
      "batch: 231\n",
      "batch: 232\n",
      "batch: 233\n",
      "batch: 234\n",
      "batch: 235\n",
      "batch: 236\n",
      "batch: 237\n",
      "batch: 238\n",
      "batch: 239\n",
      "batch: 240\n",
      "batch: 241\n",
      "batch: 242\n",
      "batch: 243\n",
      "batch: 244\n",
      "batch: 245\n",
      "batch: 246\n",
      "batch: 247\n",
      "batch: 248\n",
      "batch: 249\n",
      "batch: 250\n",
      "batch: 251\n",
      "batch: 252\n",
      "batch: 253\n",
      "batch: 254\n",
      "batch: 255\n",
      "batch: 256\n",
      "batch: 257\n",
      "batch: 258\n",
      "batch: 259\n",
      "batch: 260\n",
      "batch: 261\n",
      "batch: 262\n",
      "batch: 263\n",
      "batch: 264\n",
      "batch: 265\n",
      "batch: 266\n",
      "batch: 267\n",
      "batch: 268\n",
      "batch: 269\n",
      "batch: 270\n",
      "batch: 271\n",
      "batch: 272\n",
      "batch: 273\n",
      "batch: 274\n",
      "batch: 275\n",
      "batch: 276\n",
      "batch: 277\n",
      "batch: 278\n",
      "batch: 279\n",
      "batch: 280\n",
      "batch: 281\n",
      "batch: 282\n",
      "batch: 283\n",
      "batch: 284\n",
      "batch: 285\n",
      "batch: 286\n",
      "batch: 287\n",
      "batch: 288\n",
      "batch: 289\n",
      "batch: 290\n",
      "batch: 291\n",
      "batch: 292\n",
      "batch: 293\n",
      "batch: 294\n",
      "batch: 295\n",
      "batch: 296\n",
      "batch: 297\n",
      "batch: 298\n",
      "batch: 299\n",
      "batch: 300\n",
      "batch: 301\n",
      "batch: 302\n",
      "batch: 303\n",
      "batch: 304\n",
      "batch: 305\n",
      "batch: 306\n",
      "batch: 307\n",
      "batch: 308\n",
      "batch: 309\n",
      "batch: 310\n",
      "batch: 311\n",
      "batch: 312\n",
      "batch: 313\n",
      "batch: 314\n",
      "batch: 315\n",
      "batch: 316\n",
      "batch: 317\n",
      "batch: 318\n",
      "batch: 319\n",
      "batch: 320\n",
      "batch: 321\n",
      "batch: 322\n",
      "batch: 323\n",
      "batch: 324\n",
      "batch: 325\n",
      "batch: 326\n",
      "batch: 327\n",
      "batch: 328\n",
      "batch: 329\n",
      "batch: 330\n",
      "batch: 331\n",
      "batch: 332\n",
      "batch: 333\n",
      "batch: 334\n",
      "batch: 335\n",
      "batch: 336\n",
      "batch: 337\n",
      "batch: 338\n",
      "batch: 339\n",
      "batch: 340\n",
      "batch: 341\n",
      "batch: 342\n",
      "batch: 343\n",
      "batch: 344\n",
      "batch: 345\n",
      "batch: 346\n",
      "batch: 347\n",
      "batch: 348\n",
      "batch: 349\n",
      "batch: 350\n",
      "batch: 351\n",
      "batch: 352\n",
      "batch: 353\n",
      "batch: 354\n",
      "batch: 355\n",
      "batch: 356\n",
      "batch: 357\n",
      "batch: 358\n",
      "batch: 359\n",
      "batch: 360\n",
      "batch: 361\n",
      "batch: 362\n",
      "batch: 363\n",
      "batch: 364\n",
      "batch: 365\n",
      "batch: 366\n",
      "batch: 367\n",
      "batch: 368\n",
      "batch: 369\n",
      "batch: 370\n",
      "batch: 371\n",
      "batch: 372\n",
      "batch: 373\n",
      "batch: 374\n",
      "batch: 375\n",
      "batch: 376\n",
      "batch: 377\n",
      "batch: 378\n",
      "batch: 379\n",
      "batch: 380\n",
      "batch: 381\n",
      "batch: 382\n",
      "batch: 383\n",
      "batch: 384\n",
      "batch: 385\n",
      "batch: 386\n",
      "batch: 387\n",
      "batch: 388\n",
      "batch: 389\n",
      "batch: 390\n",
      "batch: 391\n",
      "batch: 392\n",
      "batch: 393\n",
      "batch: 394\n",
      "batch: 395\n",
      "batch: 396\n",
      "batch: 397\n",
      "batch: 398\n",
      "batch: 399\n",
      "batch: 400\n",
      "batch: 401\n",
      "batch: 402\n",
      "batch: 403\n",
      "batch: 404\n",
      "batch: 405\n",
      "batch: 406\n",
      "batch: 407\n",
      "batch: 408\n",
      "batch: 409\n",
      "batch: 410\n",
      "batch: 411\n",
      "batch: 412\n",
      "batch: 413\n",
      "batch: 414\n",
      "batch: 415\n",
      "batch: 416\n",
      "batch: 417\n",
      "batch: 418\n",
      "batch: 419\n",
      "batch: 420\n",
      "batch: 421\n",
      "batch: 422\n",
      "batch: 423\n",
      "batch: 424\n",
      "batch: 425\n",
      "batch: 426\n",
      "batch: 427\n",
      "batch: 428\n",
      "batch: 429\n",
      "batch: 430\n",
      "batch: 431\n",
      "batch: 432\n",
      "batch: 433\n",
      "batch: 434\n",
      "batch: 435\n",
      "batch: 436\n",
      "batch: 437\n",
      "batch: 438\n",
      "batch: 439\n",
      "batch: 440\n",
      "batch: 441\n",
      "batch: 442\n",
      "batch: 443\n",
      "batch: 444\n",
      "batch: 445\n",
      "batch: 446\n",
      "batch: 447\n",
      "batch: 448\n",
      "batch: 449\n",
      "batch: 450\n",
      "batch: 451\n",
      "batch: 452\n",
      "batch: 453\n",
      "batch: 454\n",
      "batch: 455\n",
      "batch: 456\n",
      "batch: 457\n",
      "batch: 458\n",
      "batch: 459\n",
      "batch: 460\n",
      "batch: 461\n",
      "batch: 462\n",
      "batch: 463\n",
      "batch: 464\n",
      "batch: 465\n",
      "batch: 466\n",
      "batch: 467\n",
      "batch: 468\n",
      "batch: 469\n",
      "batch: 470\n",
      "batch: 471\n",
      "batch: 472\n",
      "batch: 473\n",
      "batch: 474\n",
      "batch: 475\n",
      "batch: 476\n",
      "batch: 477\n",
      "batch: 478\n",
      "batch: 479\n",
      "batch: 480\n",
      "batch: 481\n",
      "batch: 482\n",
      "batch: 483\n",
      "batch: 484\n",
      "batch: 485\n",
      "batch: 486\n",
      "batch: 487\n",
      "batch: 488\n",
      "batch: 489\n",
      "batch: 490\n",
      "batch: 491\n",
      "batch: 492\n",
      "batch: 493\n",
      "batch: 494\n",
      "batch: 495\n",
      "batch: 496\n",
      "batch: 497\n",
      "batch: 498\n",
      "batch: 499\n",
      "batch: 500\n",
      "batch: 501\n",
      "batch: 502\n",
      "batch: 503\n",
      "batch: 504\n",
      "batch: 505\n",
      "batch: 506\n",
      "batch: 507\n",
      "batch: 508\n",
      "batch: 509\n",
      "batch: 510\n",
      "batch: 511\n",
      "batch: 512\n",
      "batch: 513\n",
      "batch: 514\n",
      "batch: 515\n",
      "batch: 516\n",
      "batch: 517\n",
      "batch: 518\n",
      "batch: 519\n",
      "batch: 520\n",
      "batch: 521\n",
      "batch: 522\n",
      "batch: 523\n",
      "batch: 524\n"
     ]
    }
   ],
   "source": [
    "#Here we build corpus that we will search over!\n",
    "#Fortunatly, Chroma allows metadata like DocID to be stored with embeddings, this makes analysing the quality of the retreival \n",
    "\n",
    "doc_ids = ad['DocID'].tolist()\n",
    "documents = ad['Content'].tolist()\n",
    "\n",
    "max_batch_size = 1000\n",
    "doc_batches = list(batch(documents, max_batch_size))\n",
    "id_batches = list(batch(doc_ids, max_batch_size))\n",
    "i=0\n",
    "for doc_batch, id_batch in zip(doc_batches, id_batches):\n",
    "    collection.add(\n",
    "        documents=doc_batch,\n",
    "        ids=id_batch\n",
    "    )\n",
    "    i+=1\n",
    "    print(\"batch: \"+str(i))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deletes the topics1 collection\n",
    "client.delete_collection(\"answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.peek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['692870',\n",
       "   '762348',\n",
       "   '1433100',\n",
       "   '450861',\n",
       "   '692874',\n",
       "   '1372106',\n",
       "   '387748',\n",
       "   '1015152',\n",
       "   '1097243',\n",
       "   '1298924']],\n",
       " 'embeddings': None,\n",
       " 'documents': [[' Is there any easy way to install helloworld on to my computerThe next actions you need to domake the file executable with chmod 775 helloworld from the directory where the file iscopy it over to a directory in your PATH I would suggest sudo cp helloworld usrlocalbin local since it is your local system and bin since it is a binairy And then you can dohelloworldfrom any location on the system to have it print Hello world',\n",
       "   'You should run the command as helloworldshowdev',\n",
       "   'awk printprint  helloworldtxt',\n",
       "   'echo Hello World orprintf Hello World or not for newbiesstrHello World  str  grep o str',\n",
       "   'sudo install helloworld usrlocalbin installs it to usrlocalbin read man installBtw your helloworldc should really beinclude int mainvoid    printfHello World    return 0',\n",
       "   'Try thisif  5 gt 1 thenecho Hello worldfi',\n",
       "   'First of all after compiling you are running hello but there is no such file named hello Try ls to check You have files like helloc and helloo probablyMoreover you are compiling with c flag that means source files are Compiled but do not link See man gcc to know more So even you try to run helloo will face another error bash helloo Permission deniedTo have the proper executable with link compile without c flag likecc pedantic Os helloc o helloo stdc99Next run in terminalhellooNow at output you will have Hello World',\n",
       "   'n is new line not nYou should try thisecho e hello n world',\n",
       "   'I use the command cat for this Ok so here are the contents of hellotxtHello WorldNow you open terminal and cd into the directory in which hellotxt is located and type cat hellotxt and the output will or should be Hello World',\n",
       "   'The reason is the n you have with echomd5sum  hello world6f5902ac237024bdd0c176cb93063dc4  vsecho hello world  md5sum6f5902ac237024bdd0c176cb93063dc4  When n is used a newline is not appended to the end of hello world']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [[None, None, None, None, None, None, None, None, None, None]],\n",
       " 'distances': [[1.0849459171295166,\n",
       "   1.1103765964508057,\n",
       "   1.1691514253616333,\n",
       "   1.1758662462234497,\n",
       "   1.1807368993759155,\n",
       "   1.196794867515564,\n",
       "   1.220558762550354,\n",
       "   1.222641110420227,\n",
       "   1.2335968017578125,\n",
       "   1.2366894483566284]],\n",
       " 'included': [<IncludeEnum.distances: 'distances'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.query(query_texts=\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
